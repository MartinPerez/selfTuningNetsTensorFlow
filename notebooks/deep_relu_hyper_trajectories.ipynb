{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy model hyper experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy problem setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider a simple analytical function to be learned by a network architecture.\n",
    "\n",
    "y = f(x) = x^2\n",
    "\n",
    "From the work of Weinan et al https://arxiv.org/pdf/1807.00297.pdf,  \n",
    "we expect that a Deep Relu network with:  \n",
    "- depth = L + 1  \n",
    "- width = M + d + 1\n",
    "\n",
    "where d is the dimensionality of x contained in [-1, 1], L is a positive integer and M = 2,  \n",
    "that we can eventually find a function g for which  \n",
    "- sup|g(x) - f(x)| < 2^(-2L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the work of MacKay et al as presented in https://arxiv.org/pdf/1903.03088.pdf, we can only fine tune hyperparameters that constraint the training loss and so constraint the weights/parameters of the network directly or indirectly during the training process. Examples of these type of hyperparameters can be:\n",
    "- data modification/augmentation parameters like masks applied to image data or weighting data samples\n",
    "- l1 and l2 regularization of the loss function\n",
    "- functions that apply on weight/parameters like dropout. Notice that since the method proposed operates on expectation it is possible to take into account hyperparameters with stochastic effects like dropout.\n",
    "\n",
    "Examples of hyperparameter cases not covered:\n",
    "- When the validation loss is impacted as well like by changing the number of neurons. Notice dropout is not supposed to be used during validation and the loss would not be subject ot the l1 or l2 regularizations of weights on training.\n",
    "- When the parameter is linked to the optimization procedure itself, like the learning rate of any optimizer employed.\n",
    "\n",
    "For this toy problem we propose to verify the effect of the x_range trivial hyperparameter where:\n",
    "- We have a training randomly uniformly sampled interval [-t, t]\n",
    "- To evaluate the function on equally spaced segments of an interval [-e, e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One experiment example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from self_tuning_nets.hyper.experiments.relu_toy_model import \\\n",
    "    ExperimentConfig, run_deterministic_cpu_hyper_relu_experiment\n",
    "from self_tuning_nets.visualization import function_animation\n",
    "\n",
    "from dataclasses import replace\n",
    "import matplotlib.pyplot as plt\n",
    "from self_tuning_nets.visualization import function_animation, trajectories_plot, \\\n",
    "    trajectories_legend, trajectories_dist_from_target, trajectories_general_plot\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = ExperimentConfig()\n",
    "X_eval = np.expand_dims(np.linspace(\n",
    "    -experiment_config.EVAL_RANGE,\n",
    "    experiment_config.EVAL_RANGE,\n",
    "    experiment_config.EVAL_SIZE), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_trajectory, x_range_trajectory, x_scaling_trajectory, \\\n",
    "dist_trajectory, wlosses, hlosses = \\\n",
    "run_deterministic_cpu_hyper_relu_experiment(experiment_config, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_animation(X_eval, [f_trajectory], [\"b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_SPEED = 100\n",
    "sample_seeds = range(40, 50)\n",
    "results = [\n",
    "    run_deterministic_cpu_hyper_relu_experiment(\n",
    "        replace(experiment_config,\n",
    "                FRAMEWORK_SEED=sample_seed,\n",
    "                MAX_TRAINING_CYCLES=3000,\n",
    "                PRED_SAMPLING_STEP=SAMPLING_SPEED))\n",
    "    for sample_seed in sample_seeds\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_trajectories, x_range_trajectories, x_scaling_trajectories, \\\n",
    "dist_trajectories, wlosses, hlosses = \\\n",
    "zip(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist = [min(dist) for dist in dist_trajectories]\n",
    "\n",
    "def rescaled_color(v, group, palette=\"Reds\"):\n",
    "    maxv = max(group)\n",
    "    minv = min(group)\n",
    "    if maxv == minv:\n",
    "        return plt.get_cmap(palette)(1.0)\n",
    "    return plt.get_cmap(palette)((v - minv) / (maxv - minv))\n",
    "\n",
    "g1 = [d for d in min_dist if d < 1.0]\n",
    "g2 = [d for d in min_dist if d >= 1.0]\n",
    "lines_palette = []\n",
    "for d in min_dist:\n",
    "    g = g1\n",
    "    c = \"Reds\"\n",
    "    if d >= 1.0:\n",
    "        g = g2\n",
    "        c = \"Blues\"\n",
    "    lines_palette.append(rescaled_color(d, g, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d < 1.0 for d in min_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_animation(X_eval, function_trajectories, lines_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_legend(sample_seeds, lines_palette)\n",
    "plt.gcf().set_size_inches(1.0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_general_plot(x_range_trajectories, lines_palette, ylabel=\"x_range\")\n",
    "plt.show()\n",
    "trajectories_general_plot(dist_trajectories, lines_palette, ylabel=\"Function distance from x^2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of converging hyperparameter trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_SPEED = 100\n",
    "exp_settings = list(product([42, 47], [0.5, 1.0, 2.0, 3.0]))\n",
    "results = [\n",
    "    run_deterministic_cpu_hyper_relu_experiment(\n",
    "        replace(experiment_config,\n",
    "                FRAMEWORK_SEED=sample_seed,\n",
    "                MAX_TRAINING_CYCLES=10000,\n",
    "                INIT_TRAIN_RANGE=x_range,\n",
    "                PRED_SAMPLING_STEP=SAMPLING_SPEED))\n",
    "    for sample_seed, x_range in exp_settings\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_trajectories, x_range_trajectories, x_scaling_trajectories, \\\n",
    "dist_trajectories, wlosses, hlosses = \\\n",
    "zip(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist = [min(dist) for dist in dist_trajectories]\n",
    "\n",
    "def rescaled_color(v, group, palette=\"Reds\"):\n",
    "    maxv = max(group)\n",
    "    minv = min(group)\n",
    "    if maxv == minv:\n",
    "        return plt.get_cmap(palette)(1.0)\n",
    "    return plt.get_cmap(palette)((v - minv) / (maxv - minv))\n",
    "\n",
    "g1 = [d for d in min_dist if d < 1.0]\n",
    "g2 = [d for d in min_dist if d >= 1.0]\n",
    "lines_palette = []\n",
    "for d in min_dist:\n",
    "    g = g1\n",
    "    c = \"Reds\"\n",
    "    if d >= 1.0:\n",
    "        g = g2\n",
    "        c = \"Blues\"\n",
    "    lines_palette.append(rescaled_color(d, g, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines_palette = [plt.get_cmap(\"viridis\")(i) for i in np.linspace(0, 0.7, len(function_trajectories))]\n",
    "function_animation(X_eval, function_trajectories, lines_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_legend(exp_settings, lines_palette)\n",
    "plt.gcf().set_size_inches(1.0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_general_plot(x_range_trajectories, lines_palette, ylabel=\"x_range\")\n",
    "plt.show()\n",
    "trajectories_general_plot(dist_trajectories, lines_palette, ylabel=\"Function distance from x^2\")\n",
    "plt.show()\n",
    "trajectories_general_plot(x_scaling_trajectories, lines_palette, ylabel=\"Scale x_range\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
