{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypermodel\n",
    "\n",
    "The hypermodel is simply one for which layer weights have been extended as an affine transformation of hyperparameters represented in R. Similar to a first order taylor expansion of the weights as a function of the hyperparameters.\n",
    "![title](img/hyper_model.PNG)\n",
    "\n",
    "For this we need:\n",
    "* Way to extend model weights as affine transformation of hyperparameters, like an intermediate layer or new layer\n",
    "* Clear separation of parameter weights and hyperparameter weights to compute gradients independently\n",
    "* Way to update h_space values in model (not the weights but actual values that modify model behavior like a dropout probability) (m_space hyperparameter weights should be updated by the framework when computing gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter management\n",
    "\n",
    "We need to have direct access to hyperparameter tensors to:\n",
    "* Apply invertible mappings from the hyperparameter ordinal space to R for network updates and back to the ordinal space to update the hyperparameter trajectories and compute their effects on network training. (categorical hyperparameters are not allowed, cardinality is not a requirement though and spaces can be bounded to an arbitrary interval)\n",
    "* Optimize the hyperparameters in R\n",
    "\n",
    "Hyperparameters participate in the method like:\n",
    "* Hyperparameter is created in h_space with initial values and mapped to m_space. So we have two parallel representations of the Hyperparameter.\n",
    "* The target model hyperparameter values in h_space and m_space are updated. A dropout probability would be an example of h_space and their m_space representation is perturbed and injected as input alongside X values. So the network requires both h_space and m_space representations. If hyperparameters affect the data generation process that produces X instances, then only the m_space is actually updated, depends on where we inject data manipulation code.\n",
    "* Each hyperparameter is perturbed with its own scale parameter. The latter one is also optimized after hyperparameter values are updated in the validation level of the whole optimization process.\n",
    "\n",
    "This means that we need to include in our design\n",
    "* initialize hparam\n",
    "* model update hparam\n",
    "* perturbe hparam\n",
    "* update hparam values and scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertraining step\n",
    "\n",
    "The hypertraining is a two level optimization process. As presented in \n",
    "![title](img/hyper_training_algo.PNG)\n",
    "\n",
    "For this we need:\n",
    "* Training batches\n",
    "* \\Validation batches (notice m steps of validation can be different from n steps of training, but we have the same number of \"epochs\")\n",
    "* Evaluation batch to follow function trajectory\n",
    "* Hyperparameter perturbations for all batches\n",
    "* Three optimizers:\n",
    "    * model parameter optimizer\n",
    "    * hyperparameter optimizer\n",
    "    * hp scale optimizer\n",
    "* Entropy term for validation level optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Module hypertraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model with hyper layers and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def make_unc_batch_tensor(unc_tensor, scale_tensor, perturbation, batch_size):\n",
    "    unc_repeated = tf.reshape(tf.repeat(unc_tensor, repeats=[batch_size]), (batch_size, 1))\n",
    "    return unc_repeated + tf.math.softplus(scale_tensor) * perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class HyperDense(tf.Module):\n",
    "    def __init__(\n",
    "        self, in_features, out_features,\n",
    "        unc_tensor, scale_tensor, perturbation, batch_size,\n",
    "        with_relu=True, name=\"HyperDense\"\n",
    "    ):\n",
    "        super(HyperDense, self).__init__(name=name)\n",
    "        # hyperparameters setup\n",
    "        self.unc_tensor = unc_tensor\n",
    "        self.scale_tensor = scale_tensor\n",
    "        self.perturbation = perturbation\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # layer setup\n",
    "        layer_dtype = tf.float64\n",
    "        hyper_dim = 1\n",
    "        self.with_relu = with_relu\n",
    "        \n",
    "        stdv = 1. / math.sqrt(in_features)\n",
    "        with tf.name_scope(name) as scope:\n",
    "            stdv = 1. / math.sqrt(in_features)\n",
    "            self.w = tf.Variable(tf.random.uniform([in_features, out_features], -stdv, stdv, dtype=layer_dtype),\n",
    "                                 name=\"weights\", dtype=layer_dtype)\n",
    "            self.hw = tf.Variable(tf.random.uniform([in_features, out_features], -stdv, stdv, dtype=layer_dtype),\n",
    "                                  name=\"hweights\", dtype=layer_dtype)\n",
    "            self.kw = tf.Variable(tf.random.normal([hyper_dim, 1], stddev=0.1, dtype=layer_dtype),\n",
    "                                  name=\"hkweights\", dtype=layer_dtype)\n",
    "            self.b = tf.Variable(tf.random.uniform([out_features], -stdv, stdv, dtype=layer_dtype),\n",
    "                                 name=\"bias\", dtype=layer_dtype)\n",
    "            self.hb = tf.Variable(tf.random.uniform([out_features], -stdv, stdv, dtype=layer_dtype),\n",
    "                                  name=\"hbias\", dtype=layer_dtype)\n",
    "            self.kb = tf.Variable(tf.random.normal([hyper_dim, 1], stddev=0.1, dtype=layer_dtype),\n",
    "                                  name=\"hkbias\", dtype=layer_dtype)\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, x, training=True):\n",
    "        hyper_unc_batch = make_unc_batch_tensor(self.unc_tensor, self.scale_tensor, self.perturbation, self.batch_size)\n",
    "        oy = tf.matmul(x, self.w) + self.b\n",
    "        hw = tf.linalg.matmul(hyper_unc_batch, self.kw) * tf.matmul(x, self.hw)\n",
    "        hb = tf.linalg.matmul(hyper_unc_batch, self.kb) * self.hb\n",
    "        y = oy + hw + hb\n",
    "        if self.with_relu:\n",
    "            return tf.nn.relu(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperModel(tf.Module):\n",
    "    def __init__(self, unc_tensor, scale_tensor, perturbation, batch_size,\n",
    "                 name=\"HyperModel\"):\n",
    "        super(HyperModel, self).__init__(name=name)\n",
    "        # hyperparameter setup\n",
    "        self.unc_tensor = unc_tensor\n",
    "        self.scale_tensor = scale_tensor\n",
    "        self.perturbation = perturbation\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # model setup\n",
    "        input_dim = 1\n",
    "        depth = 3  # L + 1\n",
    "        width = 4  # M + input_dim + 1\n",
    "\n",
    "        self.layers = []\n",
    "        self.layers.append(HyperDense(input_dim, width,\n",
    "                                      unc_tensor, scale_tensor, perturbation, batch_size,\n",
    "                                      with_relu=False, name=\"dense_input\"))\n",
    "        for i in range(depth):\n",
    "            self.layers.append(HyperDense(width, width,\n",
    "                                          unc_tensor, scale_tensor, perturbation, batch_size,\n",
    "                                          with_relu=True, name=(f\"hidden_{i}\")))\n",
    "        self.layers.append(HyperDense(width, 1, \n",
    "                                      unc_tensor, scale_tensor, perturbation, batch_size,\n",
    "                                      with_relu=False, name=\"dense_output\"))\n",
    "\n",
    "    def set_perturbation_var(self, perturbation):\n",
    "        self.perturbation = perturbation\n",
    "        for layer in self.layers:\n",
    "            layer.perturbation = perturbation\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, x, training=True):\n",
    "        next_input = x\n",
    "        for layer in self.layers:\n",
    "            next_input = layer(next_input, training)\n",
    "        return next_input\n",
    "\n",
    "    @tf.function\n",
    "    def update_perturbations(self, gen):\n",
    "        self.perturbation.assign(gen.normal(shape=(self.batch_size, 1), dtype=tf.float64))\n",
    "        return self.perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def logit(x):\n",
    "    return tf.math.log(x) - tf.math.log(tf.constant(1.0, dtype=tf.float64) - x)\n",
    "\n",
    "@tf.function\n",
    "def s_logit(x, min_val, max_val):\n",
    "    return logit((x - min_val)/(max_val-min_val))\n",
    "\n",
    "@tf.function\n",
    "def inv_softplus(x):\n",
    "    return tf.math.log(tf.math.exp(x) - tf.constant(1.0, dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def s_sigmoid(unc_hyperparam, min_val, max_val):\n",
    "    return (max_val - min_val) * tf.math.sigmoid(unc_hyperparam) + min_val\n",
    "\n",
    "# con_batch_tensor for bounded hyperparameter\n",
    "@tf.function\n",
    "def make_con_batch_tensor(unc_tensor, scale_tensor, perturbation, batch_size, min_val, max_val):\n",
    "    unc_batch_tensor = make_unc_batch_tensor(unc_tensor, scale_tensor, perturbation, batch_size)\n",
    "    return s_sigmoid(unc_batch_tensor, min_val, max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "@tf.function\n",
    "def entropy_term(hscale):\n",
    "    scale = tf.math.softplus(hscale)\n",
    "    return tf.math.log(scale * tf.math.sqrt(tf.constant(2.0 * math.pi * math.e, dtype=tf.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def make_training_data_tensor(unc_x_range, scale_x_range, perturbation, batch_size, min_val, max_val, gen):\n",
    "    con_x_range_batch = make_con_batch_tensor(unc_x_range, scale_x_range, perturbation, batch_size, min_val, max_val)\n",
    "    x_sample = gen.uniform(\n",
    "        shape=(batch_size, 1),\n",
    "        minval=-1, maxval=1,\n",
    "        dtype=tf.float64)\n",
    "    scaled_sample = tf.multiply(con_x_range_batch, x_sample)\n",
    "    return scaled_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup variables for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session = tf.keras.backend.clear_session\n",
    "FRAMEWORK_SEED = 39  # nice ones: 40, 42, 39; bad ones: 41\n",
    "clear_session()\n",
    "tf.random.set_seed(FRAMEWORK_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup x_range hyperparameter variables\n",
    "x_range_hyper_init = tf.constant(0.5, dtype=tf.float64)\n",
    "min_val = tf.constant(0.01, dtype=tf.float64)\n",
    "max_val = tf.constant(10, dtype=tf.float64)\n",
    "\n",
    "unc_x_range_hyper_init = s_logit(x_range_hyper_init, min_val, max_val)\n",
    "unc_x_range = tf.Variable(unc_x_range_hyper_init, name=\"unc_x_range\", dtype=tf.float64)\n",
    "scale_x_range = tf.Variable(inv_softplus(tf.constant(0.5, dtype=tf.float64)), name=\"scale_x_range\", dtype=tf.float64)\n",
    "unc_x_range, scale_x_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "gen = tf.random.Generator.from_seed(42)\n",
    "perturbation = tf.Variable(gen.normal(shape=(batch_size, 1), dtype=tf.float64), name=\"perturbation\", dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HyperModel(unc_x_range, scale_x_range, perturbation, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [v for v in model.variables if \"x_range\" not in v.name and \"perturbation\" not in v.name]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_weights = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "opt_weights = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "opt_hyper = tf.keras.optimizers.Adam(learning_rate=0.003)\n",
    "opt_scale = tf.keras.optimizers.Adam(learning_rate=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def weights_training_step():\n",
    "    # update noise\n",
    "    model.update_perturbations(gen)\n",
    "\n",
    "    # generate X\n",
    "    X_train = make_training_data_tensor(unc_x_range, scale_x_range, perturbation, batch_size, min_val, max_val, gen)\n",
    "    Y_train = X_train ** tf.constant(2.0, dtype=tf.float64)\n",
    "\n",
    "    # set tape loss\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(weights)\n",
    "        Y_pred = model(X_train, training=True)\n",
    "        loss = tf.keras.losses.MSE(Y_train, Y_pred)\n",
    "\n",
    "    # update gradient\n",
    "    grads = tape.gradient(loss, weights)\n",
    "    processed_grads = grads\n",
    "    # processed_grads = [(tf.clip_by_value(grad, -1.0, 1.0)) for grad in grads]\n",
    "    opt_weights.apply_gradients(zip(processed_grads, weights))\n",
    "    return np.sum(loss.numpy())\n",
    "\n",
    "weights_training_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup hyperparameters training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unc_x_range, scale_x_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_coefficient = tf.constant(0.001, dtype=tf.float64)\n",
    "VALIDATION_X_RANGE = 2.0\n",
    "def hyperparameters_training_step():\n",
    "    # update noise\n",
    "    model.update_perturbations(gen)\n",
    "\n",
    "    # generate X\n",
    "    X_train = gen.uniform(\n",
    "        shape=(batch_size, 1),\n",
    "        minval=-VALIDATION_X_RANGE, maxval=VALIDATION_X_RANGE,\n",
    "        dtype=tf.float64)\n",
    "    Y_train = X_train ** tf.constant(2.0, dtype=tf.float64)\n",
    "\n",
    "    # set tape loss hyper\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(unc_x_range)\n",
    "        Y_pred = model(X_train, training=True)\n",
    "        loss = tf.keras.losses.MSE(Y_train, Y_pred) - (entropy_coefficient * entropy_term(scale_x_range))\n",
    "\n",
    "    # update gradient hyper\n",
    "    grad = tape.gradient(loss, unc_x_range)\n",
    "    processed_grad = grad\n",
    "    # processed_grad = tf.clip_by_value(grad, -1.0, 1.0)\n",
    "    opt_hyper.apply_gradients(zip([processed_grad], [unc_x_range]))\n",
    "\n",
    "    # set tape loss scale\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(scale_x_range)\n",
    "        Y_pred = model(X_train, training=True)\n",
    "        loss = tf.keras.losses.MSE(Y_train, Y_pred) - (entropy_coefficient * entropy_term(scale_x_range))\n",
    "\n",
    "    # update gradient scale\n",
    "    grad = tape.gradient(loss, scale_x_range)\n",
    "    processed_grad = grad\n",
    "    # processed_grad = tf.clip_by_value(grad, -1.0, 1.0)\n",
    "    opt_scale.apply_gradients(zip([processed_grad], [scale_x_range]))\n",
    "    return np.sum(loss.numpy())\n",
    "\n",
    "hyperparameters_training_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unc_x_range, scale_x_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_sigmoid(unc_x_range, min_val, max_val).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_bounds_metric(\n",
    "    y_train: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    L: int\n",
    ") -> float:\n",
    "    # Theoretical distance used by Weinan et al in\n",
    "    # https://arxiv.org/pdf/1807.00297.pdf\n",
    "    assert len(y_train.shape) == 2  # shape (n, 1) expected\n",
    "    assert y_train.shape == y_pred.shape\n",
    "    # - pow(2, -2 * L) expected bound for validation in [-1, 1]\n",
    "    return float(np.max(np.abs(y_train - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_eval = tf.constant(np.expand_dims(np.linspace(-2.0, 2.0, 100), 1), dtype=tf.float64)\n",
    "Y_eval = X_eval ** tf.constant(2.0, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range_trajectory = []\n",
    "x_scaling_trajectory = []\n",
    "dist_trajectory = []\n",
    "preds = []\n",
    "wlosses = []\n",
    "hlosses = []\n",
    "\n",
    "MAX_EPOCHS = 10000\n",
    "WARMUP_STEPS = 5\n",
    "WEIGHT_STEPS = 2\n",
    "HYPER_STEPS = 2\n",
    "PRED_SAMPLING = 200\n",
    "\n",
    "for _ in range(WARMUP_STEPS):\n",
    "    weights_training_step()\n",
    "\n",
    "for step in tqdm(range(MAX_EPOCHS), total=MAX_EPOCHS, unit=\"batch\"):\n",
    "    # hypertraining\n",
    "    for _ in range(WEIGHT_STEPS):\n",
    "        wloss = weights_training_step()\n",
    "    for _ in range(HYPER_STEPS):\n",
    "        hloss = hyperparameters_training_step()\n",
    "    # eval and metrics\n",
    "    Y_pred = model(X_eval, training=False)\n",
    "    dist = theoretical_bounds_metric(Y_eval.numpy(), Y_pred.numpy(), 2)\n",
    "    \n",
    "    x_range_trajectory.append(s_sigmoid(unc_x_range, min_val, max_val).numpy())\n",
    "    x_scaling_trajectory.append(tf.math.softplus(scale_x_range).numpy())\n",
    "    wlosses.append(wloss)\n",
    "    hlosses.append(hloss)\n",
    "\n",
    "    if step % PRED_SAMPLING == 0:\n",
    "        preds.append(Y_pred.numpy())\n",
    "        dist_trajectory.append(dist)\n",
    "        print(\"dist: \", dist)\n",
    "    if dist < 0.05:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_range_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_scaling_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([min(d, 1) for d in dist_trajectory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(wlosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([min(l, 10) for l in hlosses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_tuning_nets.visualization import function_animation\n",
    "f_eval = X_eval.numpy()\n",
    "f_trajectory = preds\n",
    "function_animation(f_eval, [f_trajectory], [\"b\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
